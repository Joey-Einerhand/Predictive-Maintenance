{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Merged Jupyter Notebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><font color=\"green\"><h1>from file: PredictiveMaintenanceMainNotebook1</h1></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bcf959",
   "metadata": {},
   "source": [
    "# Predicitive maintanance model\n",
    "\n",
    "Made by: Joey Einerhand, CÃ©dric Cortenraede, Lennox Narinx, Giuseppe Collura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3972a9c",
   "metadata": {},
   "source": [
    "## Get file\n",
    "All files from the data directory will be loaded, _*this can take some time_\\\n",
    "based on the loaded data a dataframe will be created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9627a80",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JEAN-P~1\\AppData\\Local\\Temp/ipykernel_211472/2141052601.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Update \"data_dir\" location if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"data\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"documentation.txt\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'description.txt'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'profile.txt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Update \"data_dir\" location if necessary\n",
    "data_dir = \"data\"\n",
    "files = os.listdir(data_dir)\n",
    "files = [file for file in files if file != \"documentation.txt\" and file != 'description.txt' and file != 'profile.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "# Load the sensor data\n",
    "# Takes the mean of each row to illuminate different frequencies\n",
    "for i in range(len(files)):\n",
    "    df[files[i].strip(\".txt\")] = pd.read_csv(os.path.join(data_dir, files[i]), sep=\"\\t\", header=None, names=[files[i]]).mean(axis=1).to_numpy()\n",
    "\n",
    "# Load the profiles\n",
    "profiles = [\"Cooler condition\", \"Valve condition\", \"Internal pump leakage\", \"Hydraulic accumulator\", \"Stable flag\"]\n",
    "for i in range(len(profiles)):\n",
    "    df[profiles[i]] = pd.read_csv(os.path.join(data_dir,'profile.txt'), sep=\"\\t\", header=None)[i].to_numpy()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801a6139",
   "metadata": {},
   "source": [
    "## Analyse data\n",
    "- The analysis looks at the correlation of the different columns in comparison to each other\\\n",
    "- This gives a filter to use later\\\n",
    "- The filter will be used to determine the columns impacting for the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e03783-ecbb-4e7b-8a14-ed1419bfc300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574378f-cb75-4ad5-bf68-c4d942debec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 25))\n",
    "\n",
    "i = 1\n",
    "for column in df:\n",
    "    data = df[column]\n",
    "    plt.subplot(math.ceil(len(df.columns) / 3), 3, i)\n",
    "    \n",
    "    plt.title(column)\n",
    "    plt.hist(data)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4332c-cdc7-4e09-9a1d-e69ca957fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 25))\n",
    "\n",
    "i = 1\n",
    "for column in df:\n",
    "    data = df[column]\n",
    "    plt.subplot(math.ceil(len(df.columns) / 3), 3, i)\n",
    "    \n",
    "    plt.title(column)\n",
    "    plt.boxplot(data)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "plt.figure(figsize=(30, 10))\n",
    "sb.heatmap(corr, cmap=\"Greens\", annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be696686",
   "metadata": {},
   "source": [
    "## Making model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2216d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# Scale down the values\n",
    "X_scaler = MinMaxScaler()\n",
    "Y_scaler = MinMaxScaler()\n",
    "\n",
    "X_data = X_scaler.fit_transform(df[[\"CE\", \"CP\", \"EPS1\", \"VS1\"]])\n",
    "Y_data = Y_scaler.fit_transform(df[[\"Cooler condition\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb16ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test = train_test_split(X_data, test_size=0.2, random_state=25)\n",
    "y_train, y_test = train_test_split(Y_data, test_size=0.2, random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><font color=\"green\"><h1>from file: PredictiveMaintenanceMainNotebook2</h1></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bcf959",
   "metadata": {},
   "source": [
    "# Predictive maintanance\n",
    "\n",
    "Made by: Joey Einerhand, CÃ©dric Cortenraede, Lennox Narinx, Giuseppe Collura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e98095",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "_Loading all the text files with data to use for the prediction model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a22207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cooler condition (%)\n",
    "df_ce = pd.read_csv(\"data/CE.txt\", sep=\"\\t\", header=None, names=[\"CE\"])\n",
    "\n",
    "# Valve condition (%)\n",
    "df_se = pd.read_csv(\"data/SE.txt\", sep=\"\\t\", header=None, names=[\"SE\"])\n",
    "\n",
    "# Internal pump leakage (l/min)\n",
    "df_fs1 = pd.read_csv(\"data/FS1.txt\", sep=\"\\t\", header=None, names=[\"FS1\"])\n",
    "df_fs2 = pd.read_csv(\"data/FS2.txt\", sep=\"\\t\", header=None, names=[\"FS2\"])\n",
    "\n",
    "# Hydraulic accumulator (bar)\n",
    "df_ps1 = pd.read_csv(\"data/PS1.txt\", sep=\"\\t\", header=None, names=[\"PS1\"])\n",
    "df_ps2 = pd.read_csv(\"data/PS2.txt\", sep=\"\\t\", header=None, names=[\"PS2\"])\n",
    "df_ps3 = pd.read_csv(\"data/PS3.txt\", sep=\"\\t\", header=None, names=[\"PS3\"])\n",
    "df_ps4 = pd.read_csv(\"data/PS4.txt\", sep=\"\\t\", header=None, names=[\"PS4\"])\n",
    "df_ps5 = pd.read_csv(\"data/PS5.txt\", sep=\"\\t\", header=None, names=[\"PS5\"])\n",
    "df_ps6 = pd.read_csv(\"data/PS6.txt\", sep=\"\\t\", header=None, names=[\"PS6\"])\n",
    "\n",
    "# Temperature (Â°C)\n",
    "df_ts1 = pd.read_csv(\"data/TS1.txt\", sep=\"\\t\", header=None, names=[\"TS1\"])\n",
    "df_ts2 = pd.read_csv(\"data/TS2.txt\", sep=\"\\t\", header=None, names=[\"TS2\"])\n",
    "df_ts3 = pd.read_csv(\"data/TS3.txt\", sep=\"\\t\", header=None, names=[\"TS3\"])\n",
    "df_ts4 = pd.read_csv(\"data/TS4.txt\", sep=\"\\t\", header=None, names=[\"TS4\"])\n",
    "\n",
    "# Target Variables\n",
    "df_profile = pd.read_csv(\"data/profile.txt\", sep=\"\\t\", header=None, names=[\"CC, VC, IPL, HA, SF\"])\n",
    "\n",
    "\n",
    "# Severity grade for each type\n",
    "df_severity = pd.read_csv(\"data/profile.txt\", sep=\"\\t\", header=None, names=[\"Severity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fcf32b",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "_After loading all the data this data will be transformed to be used for the prediction model._\n",
    "\n",
    "\n",
    "1. First the sensors with multiple data points will be added to a single DataFrame.  \n",
    "2. Then all DataFrames will be added to a single DataFrame for easier access to the data.\n",
    "3. The order of columns will be changed next for a more logical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2927a12",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Make seperate dataframes from loaded data\n",
    "# Dataframe for Internal pump leakage\n",
    "df_fs = pd.DataFrame()\n",
    "\n",
    "df_fs[\"FS1\"] = df_fs1[\"FS1\"].to_numpy()\n",
    "df_fs[\"FS2\"] = df_fs2[\"FS2\"].to_numpy()\n",
    "\n",
    "# Dataframe for hydraulic accumulator\n",
    "df_ps = pd.DataFrame()\n",
    "df_ps[\"PS1\"] = df_ps1[\"PS1\"].to_numpy()\n",
    "df_ps[\"PS2\"] = df_ps2[\"PS2\"].to_numpy()\n",
    "df_ps[\"PS3\"] = df_ps3[\"PS3\"].to_numpy()\n",
    "df_ps[\"PS4\"] = df_ps4[\"PS4\"].to_numpy()\n",
    "df_ps[\"PS5\"] = df_ps5[\"PS5\"].to_numpy()\n",
    "df_ps[\"PS6\"] = df_ps6[\"PS6\"].to_numpy()\n",
    "\n",
    "# Dataframe for temperature\n",
    "df_ts = pd.DataFrame()\n",
    "df_ts[\"TS1\"] = df_ts1[\"TS1\"].to_numpy()\n",
    "df_ts[\"TS2\"] = df_ts2[\"TS2\"].to_numpy()\n",
    "df_ts[\"TS3\"] = df_ts3[\"TS3\"].to_numpy()\n",
    "df_ts[\"TS4\"] = df_ts4[\"TS4\"].to_numpy()\n",
    "\n",
    "df_cc = pd.DataFrame()\n",
    "df_profile.value_counts(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d94c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all data into a single DataFrame for easy access.\n",
    "df = pd.concat([df_fs, df_ps, df_ts], axis=1, join=\"inner\")\n",
    "df[\"CE\"] = df_ce[\"CE\"].to_numpy()\n",
    "df[\"SE\"] = df_se[\"SE\"].to_numpy()\n",
    "df[\"Severity\"] = df_severity[\"Severity\"].to_numpy()\n",
    "\n",
    "df = df.reindex(columns=[\"CE\", \"SE\", \"FS1\", \"FS2\", \"PS1\", \"PS2\", \"PS3\", \"PS4\", \"PS5\", \"PS6\", \"TS1\", \"TS2\", \"TS3\", \"TS4\", \"Severity\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6feca",
   "metadata": {},
   "source": [
    "## Predicting LSTM\n",
    "_Long Short Term Memory_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bfba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dropout,Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "plt.plot(df_ts[\"TS4\"], label=\"TS4 plot\")\n",
    "\n",
    "# Get first 400 lines to use in predicting and training\n",
    "#data = pd.DataFrame(index=range(0, 400), columns=[\"Time\", \"CE\"])\n",
    "data = pd.DataFrame(index=range(0, len(df_ts[\"TS4\"])), columns=[\"CE\",\"TS1\", \"TS2\", \"TS3\", \"TS4\"])\n",
    "for i in range(0, len(data)):\n",
    "    data[\"CE\"][i] = df[\"CE\"][i]\n",
    "    data[\"TS1\"][i] = df_ts[\"TS1\"][i]\n",
    "    data[\"TS2\"][i] = df_ts[\"TS2\"][i]\n",
    "    data[\"TS3\"][i] = df_ts[\"TS3\"][i]\n",
    "    data[\"TS4\"][i] = df_ts[\"TS4\"][i]\n",
    "\n",
    "#data[\"Time\"] = data[\"Time\"].astype(int)\n",
    "data[\"CE\"] = data[\"CE\"].astype(int)\n",
    "data[\"TS1\"] = data[\"TS1\"].astype(float)\n",
    "data[\"TS2\"] = data[\"TS2\"].astype(float)\n",
    "data[\"TS3\"] = data[\"TS3\"].astype(float)\n",
    "data[\"TS4\"] = data[\"TS4\"].astype(float)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a272c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler=MinMaxScaler(feature_range=(0,1))\n",
    "Y_scaler=MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "X_data = X_scaler.fit_transform(data[[\"TS1\", \"TS2\", \"TS3\", \"TS4\"]])\n",
    "Y_data = Y_scaler.fit_transform(data[[\"CE\"]])\n",
    "\n",
    "# split into train and test sets\n",
    "# Train is the dataset which the model is trained on\n",
    "# Test is the dataset which the model is verified with\n",
    "train_size = int(len(X_data) * 0.8)\n",
    "test_size = len(X_data) - train_size\n",
    "trainX, testX = X_data[0:train_size,:], X_data[train_size:len(X_data),:]\n",
    "trainY, testY = Y_data[0:train_size,:], Y_data[train_size:len(X_data),:]\n",
    "\n",
    "print(len(trainX), len(testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91bf33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert an array of values into a dataset matrix\n",
    "# def create_dataset(dataset, look_back=1):\n",
    "# \tdataX, dataY = [], []\n",
    "# \tfor i in range(len(dataset)-look_back-1):\n",
    "# \t\ta = dataset[i:(i+look_back), 0]\n",
    "# \t\tdataX.append(a)\n",
    "# \t\tdataY.append(dataset[i + look_back, 0])\n",
    "# \treturn np.array(dataX), np.array(dataY)\n",
    "\n",
    "# # reshape into X=t and Y=t+1\n",
    "# look_back = 1\n",
    "# trainX, trainY = create_dataset(train, look_back)\n",
    "# testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "look_back = 1\n",
    "# trainX, trainY = create_dataset(train, look_back)\n",
    "# testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851bd079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(look_back, 4)))\n",
    "#model.add(LSTM(4, return_sequences=True,stateful=True, batch_input_shape=(1, None,  look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e7c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make predictions\n",
    "# import math\n",
    "# from sklearn.metrics import mean_squared_error \n",
    "\n",
    "# trainPredict = model.predict(trainX)\n",
    "# testPredict = model.predict(testX)\n",
    "# # invert predictions\n",
    "# trainPredict = X_scaler.inverse_transform(trainPredict)\n",
    "\n",
    "# #reshape\n",
    "# trainY.reshape((len(trainY), 1))\n",
    "\n",
    "\n",
    "# trainY = scaler.inverse_transform(trainY)\n",
    "# testPredict = scaler.inverse_transform(testPredict)\n",
    "\n",
    "\n",
    "# testY = scaler.inverse_transform(testY)\n",
    "# calculate root mean squared error\n",
    "# trainScore = math.sqrt(mean_squared_error(trainY, trainPredict[:,0]))\n",
    "# print('Train Score: %.2f RMSE' % (trainScore))\n",
    "# testScore = math.sqrt(mean_squared_error(testY, testPredict[:,0]))\n",
    "# print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef5826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to predict a portion of the dataset with the trained model\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(X_data)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(X_data)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "#testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "testPredictPlot[len(trainPredict):len(X_data), :] = testPredict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "#  Convert data to something we can plot\n",
    "# ---------------------------------------\n",
    "\n",
    "def InvertScaledData(data):\n",
    "    inversedData = np.empty_like(data)\n",
    "    inversedData[:, :] = np.nan\n",
    "    for i in range(len(data)):\n",
    "        inversedData[i] = 1 - data[i]\n",
    "        \n",
    "    return inversedData\n",
    "\n",
    "newTrainPredictPlot = InvertScaledData(trainPredictPlot)\n",
    "newTestPredictPlot = InvertScaledData(testPredictPlot)\n",
    "\n",
    "# plot baseline and predictions\n",
    "# plt.plot(X_data, color=\"red\")\n",
    "# plt.plot(newTrainPredictPlot, color=\"blue\")\n",
    "# plt.plot(newTestPredictPlot, color=\"yellow\")\n",
    "plt.plot(Y_data, color=\"red\")\n",
    "plt.plot(trainPredictPlot, color=\"blue\")\n",
    "plt.plot(testPredictPlot, color=\"yellow\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------\n",
    "## METRICS\n",
    "## ----------------------------\n",
    "def timeseries_evaluation_metrics_func(y_true, y_pred):\n",
    "    def mean_absolute_percentage_error(y_true, y_pred): \n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print('Evaluation metric results:-')\n",
    "    print(f'MSE is : {metrics.mean_squared_error(y_true, y_pred)}')\n",
    "    print(f'MAE is : {metrics.mean_absolute_error(y_true, y_pred)}')\n",
    "    print(f'RMSE is : {np.sqrt(metrics.mean_squared_error(y_true, y_pred))}')\n",
    "    print(f'MAPE is : {mean_absolute_percentage_error(y_true, y_pred)}')\n",
    "    print(f'R2 is : {metrics.r2_score(y_true, y_pred)}',end='\\n\\n') \n",
    "\n",
    "print(\"Training metrics\")\n",
    "timeseries_evaluation_metrics_func(trainY, trainPredict[:,0])\n",
    "print(\"Testing metrics\")\n",
    "timeseries_evaluation_metrics_func(testY, testPredict[:,0])\n",
    "\n",
    "# trainScore = math.sqrt(mean_squared_error(trainY, trainPredict[:,0]))\n",
    "# print('Train Score: %.2f RMSE' % (trainScore))\n",
    "# testScore = math.sqrt(mean_squared_error(testY, testPredict[:,0]))\n",
    "# print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc77665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.array([2805])\n",
    "\n",
    "x = x.reshape((1, len(x), 1))\n",
    "\n",
    "ypred = model.predict(x, verbose=0)\n",
    "\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f121e58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_head = 1000\n",
    "dataset_predict = dataset\n",
    "print(dataset)\n",
    "\n",
    "dataset_predict = dataset_predict.reshape((-1))\n",
    "#print(prediction_list)\n",
    "#OriginalDataset = scaler.inverse_transform(dataset[-look_back:])\n",
    "prediction_list = dataset_predict[-look_back:]\n",
    "print(prediction_list)\n",
    "print(prediction_list[-look_back:])\n",
    "print(type(prediction_list[-look_back:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce350875",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prediction_list = prediction_list[look_back-1:]\n",
    "\n",
    "predictions_df = pd.DataFrame(index=range(len(prediction_list)), columns=[\"predictions\"])\n",
    "for i in range(len(prediction_list)):\n",
    "    predictions_df[\"predictions\"][i] = prediction_list[i]\n",
    "\n",
    "predictions_df = scaler.inverse_transform(predictions_df)\n",
    "print(predictions_df)\n",
    "# Make new dataframe for predictions so we can plot them\n",
    "# Make the index start after the baseline stops. So if baseline has index 0 to 1999,\n",
    "# The first element in predictions_df should have an index of 2000.\n",
    "results_df = pd.DataFrame(index=range(len(dataset) - 1, len(dataset) - 1 + len(prediction_list)), columns=[\"predictions\"])\n",
    "for i in range(len(predictions_df)):\n",
    "    results_df[\"predictions\"][len(dataset) - 1 + i] = predictions_df[i][0]\n",
    "print(results_df)\n",
    "# plot baseline\n",
    "dataset.reshape(1, -1)\n",
    "plt.plot(scaler.inverse_transform(dataset), color=\"red\")\n",
    "# plt.plot(trainPredictPlot, color=\"blue\")\n",
    "# plt.plot(testPredictPlot, color=\"yellow\")\n",
    "\n",
    "# Plot the future predictions\n",
    "plt.plot(results_df, color=\"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c2e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(n_head):\n",
    "    x = np.array([len(dataset) + i])\n",
    "    x = x.reshape((1, len(x), 1))\n",
    "    x = scaler.fit_transform(x)\n",
    "    ypred = model.predict(x, verbose=0)\n",
    "    results_df[\"predictions\"][len(dataset) + i] = scaler.inverse_transform(ypred)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2101f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scaler.inverse_transform(dataset), color=\"red\")\n",
    "# plt.plot(trainPredictPlot, color=\"blue\")\n",
    "# plt.plot(testPredictPlot, color=\"yellow\")\n",
    "\n",
    "# Plot the future predictions\n",
    "plt.plot(results_df, color=\"purple\")\n",
    "print(results_df)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c02e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict\n",
    "# for _ in range(n_head):\n",
    "#     x = prediction_list[-look_back:]\n",
    "#     print(\"The number to use when predicting is: \" + str(x))\n",
    "#     x = x.reshape((1, look_back, 1))\n",
    "#     print(model.predict(x)[0][0])\n",
    "#     out = model.predict(x)[0][0]\n",
    "#     print(\"The predicted number is: \" + str(out))\n",
    "#     #out = out * min(dataset_predict) + (max(dataset_predict)+min(dataset_predict))\n",
    "#     prediction_list = np.append(prediction_list, (out))\n",
    "    \n",
    "# x = pd.DataFrame(index=range(len(dataset_predict) - 1, len(dataset_predict) - 1 + n_head), columns=[\"TS4\"])\n",
    "# for i in range(len(dataset_predict), len(dataset_predict) + n_head):\n",
    "#     x[\"TS4\"].append(i)\n",
    "results_df = pd.DataFrame(index=range(len(dataset), len(dataset) - 1 + n_head), columns=[\"predictions\"])\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c501ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e204fa4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
